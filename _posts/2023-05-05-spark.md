# Spark
- Created at Berkeley in 2009
- Introduced in [2010 paper by Zaharia et al.][https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf]
- Alternative to Hadoop


## Motivation
### Why distributed computing? 


### Why Spark?
- Ideal for iterative algorithms (as many ML algs are, e.g. gradient descent) and interactive data analysis
- many computing tasks can be recast as a combination of mapping and reducing operations
- Provides APIs in several different languages: Python, R, Scala, Java, SQL

From _The Definitive Guide_:
> A cluster, or group, of computers, pool the resources of many machines together, giving us the ability to use all the cumulative resources as if they were a single computer.
> Spark manages and coordinates the execution of tasks across a cluster of computers.


### Why not Hadoop?
- 


## Physical Infrastructure
### Cluster
_cluster_ - consists of physical machines: a master node and _n_ worker nodes

## Logical Architecture
### Cluster Manager
- runs on driver node of cluster
- maintains cluster of machines that run the Spark application
- can be either Spark's built-in cluster manager, Apache Mesos, or Hadoop YARN
- note: Dataproc (Google's managed Spark/Hadoop cluster service) uses Hadoop YARN by default

## Data Structures
### Resilient Distributed Datasets
- underlie DataFrames, and other data structures
- can be accessed directly but don't have to be

### Application
_application_
  - consists of logical processes: a driver process and _n_ executor processes

_driver_
  - sits on a node in the cluster
  - runs the `main()` function
  - maintains info on Spark app
  - responds to user's program or interactive input
  - analyzes, distributes, and schedules work across executors
  - "The driver process is the heart of a Spark Application and maintains all relevant information during the lifetime of an application." (_Definitive Guide_)

_executors_
- carry out the work that the driver assigns them
- report state of the computations back to the driver (success or failure)

### Jobs, Transformations, Actions, Stages, and Tasks
- jobs
  - describe transformations on data structures
  - "break down" into 1 to n stages

- transformations
  - do not (immediately) return results
  - describe how to change a data structure
  - are used to build up a logical transformation plan
  - are not executed until triggered by an action ("lazy evaluation")

- actions
  - _always_ return results
  - trigger the execution of the logical transformation plan
  - types of actions:
    - viewing data in the console
    - collecting data to native objects in the respective language
    - writing data to output
  
- stages
  - consist of 1 to n tasks that can be executed together to compute the same operation on multiple machines
  - are initiated for shuffle operations (among other things)

- tasks
  - consist of combinations of blocks of data ("partitions") and sets of transformations that run on a single executor (i.e. a unit of computation applied to a unit of data)
  - are "pipelined" into a single stage when feeding data directly into each other without the need to move data across nodes (ex: `map` -> `filter` -> `map` pipelined into a single stage)
  - are called "shuffles" when they cross partitions, like joins, which force data to be exchanged between executor processes
  - have indices and contexts for each task


### cluster modes
- Spark can run in several different modes:
  - local - used for development mainly
  - cluster - most common real-world use
  - client - like cluster mode but with driver on local machine

## Example Spark job
maybe do a line-by-line explanation of a Spark job like on p258

## My experience with Jupyter Notebooks vs. Spark job
  
