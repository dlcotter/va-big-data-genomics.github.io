---
layout: post
title:  "#5 Telomere length estimation for MVP samples"
date:   2023-01-27 10:10:02 -0800
author: Prathima Vembu 
categories: jekyll update
---
## Overview 
- Perform telomere estimation analysis on the MVP WGS data
- We will be using [Telseq](https://github.com/zd1/telseq) to perform the estimation
- All of the updates for the Telseq project can be seen [here](https://github.com/va-big-data-genomics/mvp-telomere-analysis) 

## Project Plan 
Telseq implementation on GCP has 4 main steps <br> 

* [Read length](https://github.com/va-big-data-genomics/mvp-telomere-analysis/blob/main/scripts/read-length-estimation.txt) (r) estimation followed by setting the telomere repeat length (k) for MVP samples. From the [TopMed study](https://www.sciencedirect.com/science/article/pii/S2666979X21001051), for a read length of 150 the telomere repeat length has been set as 12. 

* [Telseq script](https://github.com/va-big-data-genomics/mvp-telomere-analysis/blob/main/scripts/telseq-script.sh) creation : This script will take the converted ```.bam``` and provide it to telseq which then estimates the telomere length which is then stored in the output text file <br>

* Dockerize Telseq : Use the [docker image](https://hub.docker.com/r/jweinstk/telseq) provided by authors or create a [new image](https://github.com/va-big-data-genomics/mvp-telomere-analysis/blob/main/Dockerfile). The main changes in the new dockerfile are changes in the version of samtools and telseq and has been pushed into dockerhub. <br>
**Note** - The current Telseq run was perfomed using the docker image generated by the authors <br>

* [Dsub script](https://github.com/va-big-data-genomics/mvp-telomere-analysis/blob/main/scripts/dsub-script-telseq.sh) to run Telseq on one ```.cram```. This script will specify the locations for the input and output files along with the Telseq script location, machine type to be used, and other dependencies <br> 

* Run the dsub job : ```bash telseq_dsub.sh```


## Approach

* The output files generated from Telseq had were populated with only 0's. None of the telomere leghts were estimated. We investigated the issue with the following steps. 

* [First](https://github.com/va-big-data-genomics/mvp-telomere-analysis/issues/1#issuecomment-1381156614) was to check if all necessary tools such as samtools, bamtools and Telseq are avaiable on the docker image.  

* [Second](https://github.com/va-big-data-genomics/mvp-telomere-analysis/issues/2#issuecomment-1381159630) we decided to check the ```samtools view``` command to make sure the ```.bam``` files were correct. To do this, instead of running telseq as a single line command, we decided to split the command into 2 lines. First to get an actual ```.bam``` file and second to input the converted ```bam``` to telseq.  This way we could make sure that the size and contents of the ```.bam``` file were as expected

* Once the bam files were created, we used [ValidateSamFile](https://github.com/va-big-data-genomics/mvp-telomere-analysis/issues/10), which is a part of the picard toolset, to check the quality of the ```.bam``` file. The bam file was empty/not fully [converted](https://github.com/va-big-data-genomics/mvp-telomere-analysis/issues/12). We fixed this issue by providing a reference ```.fasta``` and ```.fai``` file to the dsub script. For now I have specified seperate inputs for both the ```.fasta``` and ```.fai``` files, but for the next runs I plan to specify this using the INPUT_DIR tag, which will read the references recursively 
    - Region : us-west1
    - Runtime per cram : 1hour 50 minutes 
    - Machine type : n1-standard8  <br>
 
* Although the [dockerfile](https://github.com/zd1/telseq/blob/master/Dockerfile) for TelSeq is provided, we can extract the dockerfiles for images on Docker using [dfimages](https://github.com/va-big-data-genomics/mvp-telomere-analysis/issues/11). For Telseq, the dockerfile does not show how ```samtools``` was installed, and hence decoding the image is helpful. 

##  Next steps 
* Run Telseq for 10 CRAMs (ongoing)
* Results will be compared with the standalone study performed by Kruthika
* Perform quality checks 
* Scale up for production 
 
---

**Discuss** this update on our [GitHub](https://github.com/orgs/va-big-data-genomics/discussions/2)!
